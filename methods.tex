\begin{figure}
\centering \includegraphics[width=0.5\textwidth]{Figs/concept.pdf} 
\caption{ 
{\bf Schematic of numerical optimization framework.} Deep network $f$: hierarchical cascade of layers (shaded tall rectangles) of artificial neurons (circles) transforming the stimulus $x$ (bottom layer, leftmost) into high-level representations $r$ (top layer, rightmost) suitable for linear classification. Optimization framework: closed-loop algorithm that repeatedly measures the response(s) of one or multiple neuron(s), calculate the fitness, and adaptively generate stimuli to characterize the target neuron(s) via optimizing the fitness functions defined for different purposes (see Eq.~(\ref{eq:O1}--\ref{eq:S2})); when characterizing multiple neurons for population representation of given optimal or reference stimulus, response of an imaginary neuron (dashed circle) tuned to the given stimulus is optimized (see Eq.~(\ref{eq:O2}, \ref{eq:I2}, \ref{eq:S2})) equivalently.}
\label{fig:concept}
\end{figure}

Figure \ref{fig:concept} illustrates and defines the setup of the closed-loop numerical optimization framework for studying sensory representations encoded within the deep network of interest. The proposed framework consists of two main methods: (1) optimal stimulus search and (2) invariance/selectivity path search, or the first-order and quasi-second-order characterization methods, which equivalently correspond to the linear analysis and low-rank quadratic analysis (i.e.~only eigenvectors of the largest and smallest eigenvalues considered) respectively. Representation $r$, in this work, is equivalent to the response(s) of artificial neuron(s), and is denoted as unit representation (or scalar representation, i.e.~$r \in \mathbb{R}$) when referring to single neurons and population representation (or vector representation, i.e.~$r \in \mathbb{R}^R$) when referring to groups of neurons (with group size $R$). Arguably, for studying both artificial and biological neural networks, especially when evaluating task-related performances, the ability of characterizing population representations is usually more important than of characterizing unit representations, considering the differences in representational powers. This framework supports characterizing both unit (using Eq.~(\ref{eq:O1}, \ref{eq:I1}, \ref{eq:S1}), though Eq.(\ref{eq:O2}, \ref{eq:I2}, \ref{eq:S2}) can be used as well) and population (using Eq.(\ref{eq:O2}, \ref{eq:I2}, \ref{eq:S2})) representations in a unified way.

As aforementioned, this framework also treats network as non-analytical black boxes and poses no assumption about networks' properties. Nevertheless, all stimuli used in this work follow the constant energy constraint $\left\| x \right\| = E$, as suggested by previous studies where simply enhancing the contrast of a fixed stimulus can as well cause increased firing rates \cite{albrecht1982striate, cheng1994comparison}, which in most cases leads to meaningless outcomes since the spatial ``patterns'' of stimulus are not changed. For simplicity of mathematical formulation, the setting of $E=1$ is used for the rest of the paper, while in experiments $E$ is set to the average of task-related stimulus energy before normalization. In both methods, numerical optimization are executed multiple times from randomized initial settings, for two main reasons: (1) increasing robustness of the numerical solutions of, e.g., optimal stimulus and invariance/selectivity path searches (2 runs executed), and (2) providing statistical samplings of the solution spaces of, e.g., encoding specificity and invariance/selectivity subspace searches (10 and 20 runs executed). The phase of optimal stimulus search may be omitted when studying the invariance, selectivity and encoding specificities of representations of given reference stimuli.

%(STATE LOCAL VS GLOBAL OPTIMAL, WHY NAMED FIRST, Q-SECOND, WHY IN/SL ARE IMPORTANT)

\subsection*{First-Order Characterization: Optimal Stimulus Search and Analysis}

\begin{figure}
\centering \includegraphics[width=0.5\textwidth]{Figs/methods.pdf} 
\caption{
{\bf Visualization of numerical optimization procedures.} Spherical constraint: solution space of the stimulus with energy constraint $\left\| x \right\| = 1$, an $N$ dimensional sphere centered on the origin. Optimal stimulus search trajectory: best solutions (patterns with black frame boarders) from sequential search iterations with adaptive landscape modeling (gray eclipses with varying diameters); visualization following \cite{muller2010black}. Distance constraints (gray circles): solution spaces of stimuli with $0.1\pi$ to $0.5\pi$ distances from the optimal stimulus. Invariance path (dashed red curve): solutions of the invariance path searches. Selectivity path (dashed blue curve): solutions of the selectivity path searches. Target neuron in this visualization is optimally tuned to a $45^{\circ}$ Gabor pattern, invariant to phase changes, and selective to orientation changes.}
\label{fig:methods}
\end{figure}

In this work, optimal stimulus is numerically derived through the iterative optimization as \begin{equation} \label{eq:O1} \hat{x} = \underset{x} {\arg\max} f\left(x\right) = \underset{x_{g} \in \Omega \left( x_{g-1} , m_{g-1} \right)}{\arg\max} f\left(x_{g}\right),\end{equation} where $m_{g}=U\left(\left\lbrace f\left(x_{g}\right) \right\rbrace , m_{g-1}\right)$ and $1 \le g \le G$, subject to $\left\| x \right\| = 1$. The algorithm starts from random initial point $x_{0}$ on an $N$ dimensional sphere, as the noisy pattern shown in the bottom of Fig.~\ref{fig:methods}, and samples a set of $\lambda$ neighboring points (through function $\Omega$) as candidates to be evaluated, from an initial model $m_{0}$ with null distribution (in this work, multivariate Gaussian distribution starting with covariance matrix $\Sigma = I$). Through measuring the fitnesses of sampled points $\left\lbrace f\left(x_{g}\right) \right\rbrace$, the algorithm updates its model $m_{g}$ (i.e.~mean and covariance of the distribution, through function $U$) for generating samples in the following iterations, where the same operations repeat. Usually the maximally allowed iteration number $G$ is set to prevent the algorithm from running unreasonably long; nevertheless, early termination is also practically possible when optimality is reached and the neighborhood size shrinks below threshold, as shown in the center of Fig.~\ref{fig:methods}. Though the fitness landscape can be highly nonlinear, such sequential optimization procedure is usually capable of gradually accumulating knowledge of the landscape and adjusting its search directions to climb onto higher fitness areas, and the smoother the landscape is, the faster the algorithm converges. As exemplified in Fig.~\ref{fig:methods}, the ``most salient pattern'', or direction leading to high fitness areas usually can be rapidly extracted (see Fig.~\ref{fig:ind_res} for more examples). In most scenarios, fixing the maximally allowed number of fitness evaluations $\lambda G$ to, e.g., $100N$ (as adopted for the optimal stimulus search in this work) can lead to reasonably good convergence speed vs.~quality trade-off. The aforementioned sequential optimization concept is in fact commonly adopted in modern stochastic optimization \cite{spall2005introduction}, and the CMA-ES (Covariance Matrix Adaptation Evolution Strategy) algorithm \cite{hansen2001completely} is chosen as the back-end solver of this numerical framework, for its decent convergence speed and capability of handling rugged landscapes. Readers can refer to \cite{hansen2001completely} for detailed definitions of model $m$ and functions $\Omega$ and $U$. The energy constraint of stimulus is simply handled through the spherical projection before evaluation, i.e.~$f\left(p_s\left(x\right)\right)$ where $p_{s}\left(x\right) = {x} / {\left\|x\right\|}$, while the solver works unconstrainedly.

For analyzing the results of optimal stimulus searches, the following measures are adopted: (1) \emph{Spectral complexity}, which is estimated through the $L^{1}$ norm of the (2 dimensional) Fourier power spectrum of optimal stimulus, i.e.~$\left\| \mathcal{F}(\hat{x}) \right\|_{1}$, where higher value suggests higher non-sparsity (i.e.~more spectral components required to represent the signal). (2) \emph{Explanation power}, which is estimated as the mean of the linear explainability of task-related stimuli---rectified inner-product distances between a set of $n$ task-related stimuli $\left\lbrace x^{t} \right\rbrace$ and the optimal stimulus itself, i.e.~$\frac{1}{n}\sum_{i=1}^{n}\max\left(\langle x^{t}_{i} , \hat{x} \rangle , 0\right)$---and measures how well the optimal stimulus can linearly ``approximate'' task-related stimuli (or intuitively how much neuronal response might be elicited). (3) \emph{Encoding specificity}, which utilizes the optimal stimulus search as an inverse function $f^{-1}$ of a representation $r$ (i.e.~searching for the optimal stimulus of an imaginary neuron, as shown in Fig.~\ref{fig:concept}, tuned to the representation of a reference stimulus) and measures the average of structural similarities (SSIM) \cite{wang2004image} between task-related reference stimulus $x^{*}$ and $n$ reconstructed stimuli (with randomized initializations), i.e.~$\frac{1}{n}\sum_{i=1}^{n} \mathrm{SSIM}\left(x^{*} , \hat{x}^{*}_{i} \right)$ where \begin{equation} \label{eq:O2} \hat{x}^{*} = \underset{x}{\arg\max} \left( e^{-\left\|f\left(x\right)-r\right\|} \right) \end{equation} and $r = f\left(x^{*}\right)$, indicating how specific (or non-confounding) a representation $r$ is encoded.

%\underset{x}{\arg\max} f^*\left(x\right)
% ssim human vision similarity

\subsection*{Quasi-Second-Order Characterization: Invariance and Selectivity Path Search and Analysis}

\begin{figure}
\centering \includegraphics[width=0.5\textwidth]{Figs/fda.pdf} 
\caption{
{\bf Fitness-distance diagram.} Invariance (red) and selectivity (blue) curves: fitnesses of invariance and selectivity path search results plotted against distances from the optimal stimulus. Baseline (dashed gray) curve: graph of cosine function; invariance and selectivity curves of single inner-product neuron. Subsequent analyses, including path potential, subspace capacity and subspace alignment, can be performed. Invariance and selectivity path potentials can be defined on either the arccosine normalized diagram (shaded red and blue areas), or on the original diagram (line integrals).}
\label{fig:fd_diag}
\end{figure}

%\underset{x_{\delta}}{\arg\max} \hat{f}\left(x_{\delta}\right)
With respect to the optimal stimulus $\hat{x}$, the searches of invariance and selectivity paths, which consist of sets of invariant stimuli ($\left\lbrace x^{+}_{\delta} \right\rbrace$, i.e.~optimally excitatory) and selective stimuli ($\left\lbrace x^{-}_{\delta} \right\rbrace$, i.e.~optimally inhibitory) respectively, are formulated as
\begin{align}
x^{+}_{\delta} &= \underset{x_{\delta}}{\arg\max} f\left(x_{\delta}\right) \label{eq:I1} \\
&= \underset{x_{\delta}}{\arg\max} \left( e^{-\left\|f\left(x_{\delta}\right)-f\left(\hat{x}\right)\right\|} \right); \label{eq:I2} \\
x^{-}_{\delta} &= \underset{x_{\delta}}{\arg\min} f\left(x_{\delta}\right) \label{eq:S1} \\
&= \underset{x_{\delta}}{\arg\min} \left( e^{-\left\|f\left(x_{\delta}\right)-f\left(\hat{x}\right)\right\|} \right); \label{eq:S2}
\end{align}
where $0 < \delta \le \frac{\pi}{2}$, subject to $\left\| x_{\delta} \right\| = 1$ and $\langle x_{\delta} , \hat{x} \rangle = \cos\left(\delta\right)$. As visualized in Fig.~\ref{fig:methods}, the invariance path characterizes the $N$ dimensional ``curve'' that leads toward (or ideally maintains at) fitness as high as possible while moving away from the optimal stimulus, and the selectivity path characterizes the ``curve'' that leads toward fitness as low as possible while moving away. The search process is implemented as multiple runs of maximization/minimization on discretized $\delta \in \left\lbrace 0.1\pi, 0.2\pi, 0.3\pi, 0.4\pi, 0.5\pi\right\rbrace$, as the circular distance constraints shown in Fig.~\ref{fig:methods}, where each run is initialized with the result from previous run (and the $0.1\pi$ run directly with optimal stimulus $\hat{x}$) to increase the path continuity and searching speed. Each $\delta$ of both paths has a budget of $20N$ fitness evaluations which in this work appears to be practically sufficient. The maximization and minimization simply use the same back-end solver, and the linear constraint $\langle x_{\delta} , \hat{x} \rangle = \cos\left(\delta\right)$ can be easily handled through an extra conic projection before evaluation, i.e.~$f\left(p_c\left(p_s\left(x\right)\right)\right)$ where $p_c\left(x\right) = \cos\left(\delta\right)\hat{x} + \sin\left(\delta\right)\dot{x} \mathbin{/} \left\|\dot{x}\right\|$ and $\dot{x} = x - \langle\hat{x},x\rangle \hat{x}$. The way the simple linear constraint is constructed to enforce the exploration of a larger extent of the fitness landscape is the one of the main differences compared to \cite{erhan2010understanding}.
In this work, the distance constraint $\delta$ only goes up to $\frac{\pi}{2}$, since on the $N$ dimensional sphere, stimuli fall in $\frac{\pi}{2} < \delta \le \pi$ are simply ``negatives'' of those in $0 < \delta \le \frac{\pi}{2}$, which are of less uniqueness and interest; nevertheless, going up to the full range $0 < \delta \le \pi$ is numerically supported.

For analyzing the results of invariance/selectivity path searches, the following measures are adopted: (1) \emph{Path potential}: while the search results of invariance/selectivity paths can be visualized via the \emph{fitness-distance diagram} \cite{jones1995fitness} as shown in Fig.~\ref{fig:fd_diag} where perfect invariance and selectivity are flat lines at highest and lowest fitnesses respectively, the baselines of invariance and selectivity also can be intuitively (and analytically) defined as paths of the simplest form of neural networks---an inner-product neuron, $f(x) = w^{T}x$---which precisely overlap and follow the monotonic cosine falloff from its optimal stimulus $\hat{x}=w$, as $f(x_{\delta}) = \cos(x_{\delta})$ by definition; the invariance (and selectivity, similarly) path potential of a unit representation can thus be defined as $\int_{0}^{\frac{\pi}{2}}{\left| \cos^{-1}\left(f\left( x^{+}_{\delta} \right)\right) - \delta \right|}\mathrm{d}\delta \mathbin{/} {\frac{\pi}{2}}$, the area sandwiched between the invariance (and selectivity) and baseline curves (in $\cos^{-1}$ domain such that both potential values fall in the range $\left[0,1\right]$), measuring how invariant (and selective) a target neuron is compared to the baseline (i.e.~zero invariance/selectivity potential); for population representation where no baseline can be easily identified, the invariance (and selectivity) path potential is alternatively defined as $\int_{0}^{\frac{\pi}{2}}{\exp\left(-\left\|f\left(x^{+}_{\delta}\right)-f\left(\hat{x}\right)\right\|\right)}\mathrm{d}\delta \mathbin{/} {\frac{\pi}{2}}$. (2) \emph{Subspace capacity}: compared to path potential, which is designed to characterize the best invariance/selectivity path even when only one of such exists, subspace capacity estimates the ``dimensionality'' (i.e.~how diverse different paths can be) of the linear subspace formed by multiple path search results via the nuclear norm of concatenation of $n$ results $\left\|\left[x_{\delta,1},\dots,x_{\delta,n}\right]\right\|_{*}$ (in this work $n=20$ runs at $\delta=0.1\pi$). (3) \emph{Subspace alignment}, which measures the alignment between subspaces formed by task-related stimuli and invariance/selectivity paths (i.e.~how likely the invariance/selectivity can benefit, e.g., stimulus recognition) via estimating the sparsity of $n$ path search results projected onto the principal component vectors $V$ of task-related stimuli $\left\lbrace x^{t} \right\rbrace$, i.e.~$\frac{1}{n}\sum_{i=1}^{n} \left\| Vx_{\delta,i} \right\|_{1}$ ($n=20$ runs at $\delta=0.1\pi$ as well). %SUBSPACE OVERLAP?

